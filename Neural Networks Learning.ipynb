{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise 4: Neural Networks Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import scipy\n",
    "from random import randint\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import normalize\n",
    "from math import sqrt\n",
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y', 'X', '__version__', '__header__', '__globals__']\n",
      "['Theta2', '__version__', '__header__', 'Theta1', '__globals__']\n",
      "\n",
      "Random sample\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABECAYAAACRbs5KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLeiGYRgxgi3ohmEYMYIt6IZhGDHCfwHoA1ZZZ0CwOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110c609d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadmat('ex4data1.mat')\n",
    "weights = loadmat('ex4weights.mat')\n",
    "print data.keys()\n",
    "print weights.keys()\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "Theta1 = weights['Theta1']\n",
    "Theta2 = weights['Theta2']\n",
    "\n",
    "rand = randint(0, 5000)\n",
    "sample = X[np.random.choice(X.shape[0], 10, replace=False), :]\n",
    "\n",
    "print '\\nRandom sample'\n",
    "plt.imshow(sample.reshape((-1, 20)).T, cmap='gray_r')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Sigmoid and Sigmoid Gradient Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 0 sigmoid gradient is 0.25\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~imtoobose/28.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z)*(1- sigmoid(z))\n",
    "\n",
    "Xaxis = np.arange(-10, 10)\n",
    "\n",
    "print 'At 0 sigmoid gradient is', sigmoid_gradient(0)\n",
    "print ''\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=Xaxis,\n",
    "    y=sigmoid(Xaxis),\n",
    "    name='Sigmoid'\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x=Xaxis,\n",
    "    y=sigmoid_gradient(Xaxis),\n",
    "    name='Sigmoid Gradient',\n",
    ")\n",
    "\n",
    "py.iplot([trace1, trace2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Get the numerical gradient (for checking how well back propagation is working)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def computeNumericalGradient(J, params, input_layer_size, hidden_layer_size, num_labels, X, y, lamb):\n",
    "    e = 1e-4\n",
    "    perturb = np.zeros((params.shape[0], params.shape[0]))\n",
    "    perturb[np.arange(params.shape[0]), np.arange(params.shape[0])] = e\n",
    "    \n",
    "    loss1 = np.zeros(params.shape)\n",
    "    loss2 = np.zeros(params.shape)\n",
    "    \n",
    "    for i in range(params.shape[0]):\n",
    "        loss1[i] = J(params - perturb[i], input_layer_size, hidden_layer_size, num_labels, X, y, lamb)[0]\n",
    "        loss2[i] = J(params + perturb[i], input_layer_size, hidden_layer_size, num_labels, X, y, lamb)[0]\n",
    "    \n",
    "    return (loss2-loss1)/(2*e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Net functions\n",
    "\n",
    "### randInitializeWeights:\n",
    "**Params:**\n",
    "- L_in : size of incoming layer\n",
    "- L_out: size of outgoing layer\n",
    "\n",
    "**Return**\n",
    "- numgrad: the numerical gradient\n",
    "\n",
    "### nnCostFunction\n",
    "**Params:**\n",
    "- nn_params : 1D vector with parameters for neural net\n",
    "- input_layer_size: --\n",
    "- hidden_layer_size: --\n",
    "- num_labels: number of labels or size of final layer\n",
    "- X: input data \n",
    "- y: classes\n",
    "- lamb: regularization coefficient\n",
    "\n",
    "**Return**\n",
    "- J: the cost h0 for the neural net with current parameters\n",
    "- gradient from back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta1\t(25, 401)\n",
      "Theta2\t(10, 26)\n",
      "X\t(5000, 400)\n",
      "\n",
      "Cost (lambda = 0)\t0.287629165161\n",
      "Cost (lambda = 1)\t0.383769859091\n",
      "\n",
      "Gradient (lambda = 0)\n",
      "[  6.18712766e-05   0.00000000e+00   0.00000000e+00 ...,   9.66104721e-05\n",
      "  -7.57736846e-04   7.73329872e-04]\n",
      "(10285,)\n"
     ]
    }
   ],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    ep_init = sqrt(6)/sqrt(L_in + L_out)\n",
    "    \n",
    "    return np.random.uniform(-ep_init, ep_init, size=(L_out, L_in))\n",
    "        \n",
    "\n",
    "def create_nn_model(Theta1, Theta2, X):\n",
    "    XX = np.append(np.ones((m, 1)), X, axis=1)\n",
    "    h = sigmoid(Theta1.dot(XX.T))\n",
    "    l1 = np.append(np.ones((1, h.shape[1])), h, 0)\n",
    "    h1 = sigmoid(Theta2.dot(l1))\n",
    "    ans = np.argmax(h1, axis=0) + (np.repeat(1, h1.shape[1]))\n",
    "    \n",
    "    return ans\n",
    "\n",
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamb):\n",
    "    # Recreate Theta1 and Theta2 (this is straight from the provided Matlab code)\n",
    "    Theta1 = nn_params[0: hidden_layer_size * (input_layer_size + 1)].reshape(\n",
    "                 ((hidden_layer_size, (input_layer_size + 1))))\n",
    "\n",
    "    Theta2 = nn_params[((hidden_layer_size * (input_layer_size + 1))):].reshape(\n",
    "                     (num_labels, (hidden_layer_size + 1)) )\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    Theta1_grad = np.zeros_like(Theta1.shape)\n",
    "    Theta2_grad = np.zeros_like(Theta2.shape)\n",
    "    \n",
    "    \n",
    "    # One hidden layer\n",
    "    # Forward feedback through layers\n",
    "    a1 = np.append(np.ones((m, 1)), X, axis=1)\n",
    "    a2 = sigmoid(Theta1.dot(a1.T))\n",
    "    a2_ = np.append(np.ones((1, a2.shape[1])), a2, axis=0)\n",
    "    a3 = sigmoid(Theta2.dot(a2_))\n",
    "    \n",
    "    # Remove 1 from each value in y because python is 0 indexed\n",
    "    yd = (y - 1)\n",
    "    \n",
    "    # Create a Y vector of vectors where each vector has a 1 \n",
    "    # for the corresponding class in y, else 0\n",
    "    Y = (np.zeros(a3.shape))\n",
    "    Y[ yd.ravel(), np.arange(Y.shape[1])] = 1\n",
    "    \n",
    "    # Create Thetas without the bias value (Theta0)\n",
    "    Theta1_0 = np.delete(Theta1, 0, axis=1)\n",
    "    Theta2_0 = np.delete(Theta2, 0, axis=1)\n",
    "    \n",
    "    # Regularized term\n",
    "    reg = (lamb/(2*m))*(np.sum(np.square(Theta1_0)) + np.sum(np.square(Theta2_0)))\n",
    "    \n",
    "    # Cost\n",
    "    J = (-1/m) * np.sum((np.log(a3)) * Y +  (np.log(1 - a3)) * (1- Y)) + reg\n",
    "    \n",
    "    # Back propagation gradient\n",
    "    delta_3 = (a3 - Y)\n",
    "    delta_2 = Theta2.T[1:,:].dot(delta_3) * sigmoid_gradient(Theta1.dot(a1.T))\n",
    "    \n",
    "    grad_2 = delta_3.dot(a2_.T)/m\n",
    "    grad_1 = delta_2.dot(a1)/m\n",
    "    \n",
    "    if np.isnan(J):\n",
    "        J = np.inf\n",
    "        \n",
    "    return J, np.r_[grad_1.ravel(), grad_2.ravel()]\n",
    "    \n",
    "    \n",
    "    \n",
    "m, n = X.shape\n",
    "\n",
    "print 'Theta1\\t', Theta1.shape\n",
    "print 'Theta2\\t', Theta2.shape\n",
    "print 'X\\t', X.shape\n",
    "\n",
    "nn_params = np.r_[Theta1.ravel(), Theta2.ravel()]\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "\n",
    "cost, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, 0)\n",
    "cost1, grad1 = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, 1)\n",
    "\n",
    "print '\\nCost (lambda = 0)\\t', cost\n",
    "print 'Cost (lambda = 1)\\t', cost1\n",
    "print '\\nGradient (lambda = 0)'\n",
    "print grad\n",
    "print grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Testing out the quality of back propagation with a limited dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference is: 3.01688213259e-10\n"
     ]
    }
   ],
   "source": [
    "testCost, testGrad = nnCostFunction(\n",
    "                        nn_params, \n",
    "                        input_layer_size, \n",
    "                        hidden_layer_size, \n",
    "                        num_labels, \n",
    "                        X[0:30], y[0:30], 0\n",
    "                    )\n",
    "\n",
    "numgrad = computeNumericalGradient(\n",
    "            nnCostFunction, \n",
    "            nn_params, \n",
    "            input_layer_size, \n",
    "            hidden_layer_size, \n",
    "            num_labels, \n",
    "            X[0:30], \n",
    "            y[0:30], \n",
    "            0)\n",
    "\n",
    "\n",
    "diff = np.linalg.norm(numgrad.ravel() - testGrad.ravel()) / np.linalg.norm(numgrad.ravel() + testGrad.ravel())\n",
    "print 'Difference is: ' + str(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Train the neural net with new values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.03719797 -0.04776479 -0.06487183 ..., -2.33313726 -8.22902677\n",
      "  4.35837358] (8230,)\n"
     ]
    }
   ],
   "source": [
    "input_layer_size = 400\n",
    "hidden_layer_size = 20\n",
    "num_labels = 10\n",
    "\n",
    "init_params1 = randInitializeWeights(hidden_layer_size, input_layer_size + 1)\n",
    "init_params2 = randInitializeWeights(num_labels, hidden_layer_size + 1)\n",
    "\n",
    "init_params = np.r_[init_params1.ravel(), init_params2.ravel()]\n",
    "\n",
    "new_params = minimize(\n",
    "                nnCostFunction, \n",
    "                init_params, \n",
    "                args=(input_layer_size, hidden_layer_size, num_labels, X, y, 0),\n",
    "                options={'maxiter':4000},\n",
    "                jac=True,\n",
    "                method='CG'\n",
    "            )\n",
    "\n",
    "print new_params.x, new_params.x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Check the accuracy and predictions of the new theta values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is:\t0.0014332350753\n",
      "Accuracy is:\t100.0%\n"
     ]
    }
   ],
   "source": [
    "input_layer_size = 400\n",
    "hidden_layer_size = 20\n",
    "num_labels = 10\n",
    "n_params = new_params.x\n",
    "\n",
    "cost, grad = nnCostFunction(n_params, input_layer_size, hidden_layer_size, num_labels, X, y, 0)\n",
    "nTheta1    = n_params[0: hidden_layer_size * (input_layer_size + 1)].reshape(\n",
    "                 ((hidden_layer_size, (input_layer_size + 1))))\n",
    "nTheta2    = n_params[((hidden_layer_size * (input_layer_size + 1))):].reshape(\n",
    "                 (num_labels, (hidden_layer_size + 1)) )\n",
    "\n",
    "model = create_nn_model(nTheta1, nTheta2, X)\n",
    "acc   = model[model == y.ravel()].shape[0]/float(model.shape[0])\n",
    "\n",
    "print 'Cost is:\\t', cost\n",
    "print 'Accuracy is:\\t' + str(acc*100) + '%'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
